{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes pillow gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:03:56.874366Z","iopub.execute_input":"2026-02-17T15:03:56.874561Z","iopub.status.idle":"2026-02-17T15:04:06.538809Z","shell.execute_reply.started":"2026-02-17T15:03:56.874540Z","shell.execute_reply":"2026-02-17T15:04:06.537981Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0rc2 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:04:13.677076Z","iopub.execute_input":"2026-02-17T15:04:13.677373Z","iopub.status.idle":"2026-02-17T15:04:13.682036Z","shell.execute_reply.started":"2026-02-17T15:04:13.677342Z","shell.execute_reply":"2026-02-17T15:04:13.681430Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\nfrom transformers import BitsAndBytesConfig\nfrom PIL import Image\nimport torchvision.transforms as T\nimport gradio as gr\nimport gc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:04:18.236704Z","iopub.execute_input":"2026-02-17T15:04:18.237018Z","iopub.status.idle":"2026-02-17T15:04:52.551036Z","shell.execute_reply.started":"2026-02-17T15:04:18.236991Z","shell.execute_reply":"2026-02-17T15:04:52.550446Z"}},"outputs":[{"name":"stderr","text":"2026-02-17 15:04:33.160499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771340673.399410      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771340673.466628      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771340674.041887      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771340674.041943      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771340674.041948      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771340674.041951      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\nprocessor = AutoProcessor.from_pretrained(model_name)\n\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:05:04.314266Z","iopub.execute_input":"2026-02-17T15:05:04.314578Z","iopub.status.idle":"2026-02-17T15:07:57.022232Z","shell.execute_reply.started":"2026-02-17T15:05:04.314551Z","shell.execute_reply":"2026-02-17T15:07:57.021427Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f91729512084d079b1e1d8564996822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4afeaedea1475cbaa1ef65ec348a68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eae8608ec984a3187b6c217254dbeaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd82c7344cd4c4e9aede1ace2e57963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8788a9c862b940a6928c072d2f1cbbd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bb293674ee94db6bfb98c2ccb43051d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ce561b577f42beb26eabd51844e76d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac4388133674a06bb073bf1c3189800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7fda1c5886a473fa08e3946fc0a9bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a302c4417c48359480b9b3e3fe54c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15172f9b2fe4f7cb42dfa977a213d46"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28dbdd94eecb4c399095f06d15e5be64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8552a924116b4070bf31126b3af9ce22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58a355505b9340da841ee03c686a945c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1858f201ed44cf8aec7efc56ab0fcfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a779cfa5d5544f9085d5aa2d4dd54a49"}},"metadata":{}},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"prompt = \"\"\"\nYou are a professional jewelry authentication expert.\n\nYour task is to compare two jewelry images and output ONLY a final similarity score (0â€“100).\n\nCRITICAL RULES:\n- Ignore lighting, shadows, background, hand presence, scale ruler, brightness, camera quality, and image resolution.\n- Mentally isolate the jewelry from the background before comparison.\n- Catalogue images may contain multiple items or multiple views.\n- If multiple items exist, compare ONLY the pair that shares the dominant matching design.\n- Do NOT average across different pieces.\n\n--------------------------------------------------\nSTEP 1 â€” JEWELRY TYPE CHECK (MANDATORY FIRST)\n--------------------------------------------------\n\nIdentify the primary jewelry type in each image.\n\nIf the primary jewelry types are clearly different:\nâ†’ Immediately assign a score between 0 and 20.\nâ†’ STOP comparison.\n\nIf types match, continue.\n\n--------------------------------------------------\nSTEP 2 â€” OUTER SILHOUETTE GATE\n--------------------------------------------------\n\nIf outer silhouette clearly differs:\nâ†’ Final score must be below 40.\n\nIf silhouette matches:\nâ†’ Continue.\nDo NOT later penalize for the same structural difference again.\n\n--------------------------------------------------\nSTEP 3 â€” CORE STRUCTURE\n--------------------------------------------------\n\nIf core structure clearly differs:\nâ†’ Subtract 25.\n\nNever apply more than ONE major structural penalty.\n\n--------------------------------------------------\nSTEP 4 â€” MATERIAL / METAL CHECK\n--------------------------------------------------\n\nIf metal color is clearly different:\nâ†’ Subtract 25.\n\nIf one is two-tone and other single-tone:\nâ†’ Subtract 15.\n\nMaterial difference should NOT reduce score below 50\nif structure is otherwise identical.\n\n--------------------------------------------------\nSTEP 5 â€” DETAILED FEATURE SCORING\n--------------------------------------------------\n\nStart from 100.\n\nSubtract ONLY for clearly visible mismatches:\n\n- 20 if center stone shape differs\n- 15 if stone count differs\n- 10 for symmetry/layout differences\n- 10 for proportion differences\n- 5â€“10 for small visible variation\n\n--------------------------------------------------\nSTABILITY RULES\n--------------------------------------------------\n\nIf type, silhouette, and structure match:\nâ†’ Score must NOT fall below 60 unless material differs.\n\nIf same design + same metal:\nâ†’ Score should be 85â€“100.\n\n--------------------------------------------------\nOUTPUT RULE\n--------------------------------------------------\n\nReturn ONLY the final similarity score number (0â€“100).\nDo not explain.\nOnly output the number.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:08:06.918565Z","iopub.execute_input":"2026-02-17T15:08:06.919873Z","iopub.status.idle":"2026-02-17T15:08:06.924788Z","shell.execute_reply.started":"2026-02-17T15:08:06.919838Z","shell.execute_reply":"2026-02-17T15:08:06.924070Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def compare_images(image1, image2):\n\n    torch.cuda.empty_cache()\n\n    # Resize SAME as your notebook (1024)\n    transform = T.Resize((1024, 1024))\n    image1 = transform(image1.convert(\"RGB\"))\n    image2 = transform(image2.convert(\"RGB\"))\n\n    with torch.no_grad():\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": prompt}\n                ]\n            }\n        ]\n\n        text = processor.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = processor(\n            text=[text],\n            images=[image1, image2],\n            return_tensors=\"pt\"\n        ).to(model.device)\n\n        # ðŸ”¥ Memory-optimized generation\n        output = model.generate(\n            **inputs,\n            max_new_tokens=80,      # reduced from 300 (huge memory saver)\n            temperature=0.2,\n            do_sample=False,\n            use_cache=False         # critical for VRAM stability\n        )\n\n        response = processor.decode(\n            output[0],\n            skip_special_tokens=True\n        )\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return response.strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:09:13.958368Z","iopub.execute_input":"2026-02-17T15:09:13.959004Z","iopub.status.idle":"2026-02-17T15:09:13.965373Z","shell.execute_reply.started":"2026-02-17T15:09:13.958963Z","shell.execute_reply":"2026-02-17T15:09:13.964610Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import gradio as gr\n\n# Your existing backend function\n# def compare_images(img1, img2):\n#     ...\n#     return similarity_score\n\ndemo = gr.Interface(\n    fn=compare_images,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Upload Jewelry Image 1\"),\n        gr.Image(type=\"pil\", label=\"Upload Jewelry Image 2\"),\n    ],\n    outputs=gr.Textbox(label=\"Similarity Score\"),\n    title=\"AI Jewelry Authentication System (7B VLM)\",\n    description=\"Quantized 7B Vision-Language Model running on Kaggle GPU\"\n)\n\n# For latest Gradio (4.x+)\ndemo.queue()\ndemo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:12:15.545680Z","iopub.execute_input":"2026-02-17T15:12:15.546492Z","iopub.status.idle":"2026-02-17T15:12:17.184553Z","shell.execute_reply.started":"2026-02-17T15:12:15.546453Z","shell.execute_reply":"2026-02-17T15:12:17.183998Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://be78fb331061e7f63d.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://be78fb331061e7f63d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"}],"execution_count":22}]}